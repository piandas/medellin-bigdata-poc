{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a43319ef-6685-48e1-ba85-70a9f66457e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 1: Imports y SparkSession\n",
    "# ==============================\n",
    "from pathlib import Path\n",
    "import json, random, uuid, time\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, expr, floor, rand, to_timestamp,\n",
    "    date_format, year, month, dayofmonth, hour, minute, second,\n",
    "    struct, to_json\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StringType, IntegerType, DoubleType, StructType, StructField,\n",
    "    TimestampType\n",
    ")\n",
    "from shapely.geometry import Point, shape\n",
    "\n",
    "# Inicia Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"01_setup_load_sparkified\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Carga configuración\n",
    "cfg      = json.loads(Path(\"/Workspace/Users/santiagobustosp@gmail.com/medellin-bigdata-poc/notebooks/1_simulation/sim_config.json\").read_text())\n",
    "base     = Path(cfg[\"base_path\"])\n",
    "paths    = cfg[\"paths\"]\n",
    "interval = cfg[\"interval_seconds\"]\n",
    "qty_min, qty_max = cfg[\"quantity_range\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3314a0d-1f97-4059-8728-b86bc0c27757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 2: Leer insumos (GeoPandas + Spark)\n",
    "# ==============================\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Ruta local al repo (GeoPandas sí puede acceder)\n",
    "raw_dir = \"/Workspace/Users/santiagobustosp@gmail.com/medellin-bigdata-poc/data/raw\"\n",
    "\n",
    "# 2) Leer barrios y máscara con GeoPandas\n",
    "gdf_neigh = gpd.read_parquet(f\"{raw_dir}/medellin_neighborhoods.parquet\")\n",
    "gdf_mask  = gpd.read_parquet(f\"{raw_dir}/50001.parquet\")\n",
    "\n",
    "# 3) Convertir a listas para UDF espacial\n",
    "neigh_list = gdf_neigh.to_dict(\"records\")\n",
    "mask_geom  = shape(gdf_mask.loc[0, \"geometry\"])\n",
    "\n",
    "# 4) Leer clientes y empleados con Spark (Spark sí puede acceder si están en DBFS o S3, pero no en /Workspace)\n",
    "# Solución: también leerlos con pandas si están en /Workspace\n",
    "pdf_cust = pd.read_parquet(f\"{raw_dir}/customers.parquet\")\n",
    "pdf_emp  = pd.read_parquet(f\"{raw_dir}/employees.parquet\")\n",
    "\n",
    "# 5) Convertir a Spark\n",
    "cust_df = spark.createDataFrame(pdf_cust)\n",
    "emp_df  = spark.createDataFrame(pdf_emp)\n",
    "\n",
    "print(f\"✅ Barrios: {len(neigh_list)} | Clientes: {cust_df.count()} | Empleados: {emp_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2002ee18-2ccf-45d4-9411-81837e6e074e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 3: Simulación de eventos con estructura final deseada\n",
    "# ==============================\n",
    "from shapely.geometry import Point\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Número de eventos a generar\n",
    "N = 20\n",
    "\n",
    "# IDs preparados para sampling\n",
    "cust_ids = pdf_cust[\"customer_id\"].tolist()\n",
    "emp_ids  = pdf_emp[\"employee_id\"].tolist()\n",
    "\n",
    "# Generar lista de eventos con estructura exacta\n",
    "events = []\n",
    "for _ in range(N):\n",
    "    b = random.choice(neigh_list)\n",
    "    minx, miny, maxx, maxy = shape(b[\"geometry\"]).bounds\n",
    "    while True:\n",
    "        lon = random.uniform(minx, maxx)\n",
    "        lat = random.uniform(miny, maxy)\n",
    "        pt = Point(lon, lat)\n",
    "        if shape(b[\"geometry\"]).contains(pt) and mask_geom.contains(pt):\n",
    "            break\n",
    "    event = OrderedDict([\n",
    "        (\"latitude\",           lat),\n",
    "        (\"longitude\",          lon),\n",
    "        (\"date\",               datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")),\n",
    "        (\"customer_id\",        random.choice(cust_ids)),\n",
    "        (\"employee_id\",        random.choice(emp_ids)),\n",
    "        (\"quantity_products\",  random.randint(qty_min, qty_max)),\n",
    "        (\"order_id\",           str(uuid.uuid4()))\n",
    "    ])\n",
    "    events.append(event)\n",
    "\n",
    "# Crear DataFrame Spark desde los eventos\n",
    "schema_ev = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), False),\n",
    "    StructField(\"longitude\", DoubleType(), False),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"employee_id\", IntegerType(), False),\n",
    "    StructField(\"quantity_products\", IntegerType(), False),\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "])\n",
    "df_raw = spark.createDataFrame(events, schema=schema_ev)\n",
    "print(f\"✅ Generados {df_raw.count()} eventos con estructura correcta\")\n",
    "df_raw.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76eac53-c3dd-4c0d-8cc8-068762b83f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 3.1: Registrar cada evento simulado en su propio JSON\n",
    "# ==============================\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# 1) Timestamp único para esta corrida\n",
    "run_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 2) Carpeta destino en tu repo\n",
    "#    base es Path(cfg[\"base_path\"]) → \"/Workspace/Users/.../medellin-bigdata-poc\"\n",
    "output_base = Path(base, \"data\", \"sim-events\", run_ts)\n",
    "\n",
    "# 3) Creo la carpeta (no sobreescribe ejecuciones anteriores)\n",
    "output_base.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "# 4) Escribo cada evento en un archivo JSON separado\n",
    "for ev in events:\n",
    "    order_id = ev[\"order_id\"]\n",
    "    file_path = output_base / f\"{order_id}.json\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(ev, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ {len(events)} archivos JSON de eventos escritos en: {output_base}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5842fe37-8b8e-43a6-9f14-7dc16eae08a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 4: Spatial‐join con UDF (Shapely) — con district y neighborhood\n",
    "# ==============================\n",
    "from shapely.geometry import Point, shape\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# UDF que asigna el código de barrio (district) según lat/lon\n",
    "def find_district(lat, lon):\n",
    "    pt = Point(lon, lat)\n",
    "    for b in neigh_list:\n",
    "        if shape(b[\"geometry\"]).contains(pt):\n",
    "            return b[\"IDENTIFICACION\"]\n",
    "    return None\n",
    "\n",
    "find_district_udf = udf(find_district, StringType())\n",
    "\n",
    "# UDF que asigna el nombre de barrio (neighborhood) según lat/lon\n",
    "def find_neighborhood_name(lat, lon):\n",
    "    pt = Point(lon, lat)\n",
    "    for b in neigh_list:\n",
    "        if shape(b[\"geometry\"]).contains(pt):\n",
    "            return b[\"NOMBRE\"]\n",
    "    return None\n",
    "\n",
    "find_name_udf = udf(find_neighborhood_name, StringType())\n",
    "\n",
    "# Aplicamos ambos UDFs para enriquecer df_raw\n",
    "df_events = (\n",
    "    df_raw\n",
    "      .withColumn(\"district\", find_district_udf(\"latitude\", \"longitude\"))\n",
    "      .withColumn(\"neighborhood\", find_name_udf(\"latitude\", \"longitude\"))\n",
    "      .drop(\"neigh_id\")\n",
    ")\n",
    "\n",
    "# Verificamos resultado\n",
    "print(\"✅ Spatial‐join completo: código y nombre de barrio asignados\")\n",
    "df_events.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc9b3cd-f146-45cc-a93f-e073a301abe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 5: Transformar a esquema Bronze con columnas reordenadas\n",
    "# ==============================\n",
    "from pyspark.sql.functions import (\n",
    "    to_timestamp, date_format,\n",
    "    year, month, dayofmonth, hour, minute, second,\n",
    "    col\n",
    ")\n",
    "\n",
    "df_bronze = (\n",
    "    df_events\n",
    "      # Convertir la fecha string a timestamp para extraer componentes\n",
    "      .withColumn(\"event_ts\", to_timestamp(\"date\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "      # Formatear partition_date como ddMMyyyy\n",
    "      .withColumn(\"partition_date\", date_format(\"event_ts\", \"ddMMyyyy\"))\n",
    "      # Desglosar componentes de fecha/hora\n",
    "      .withColumn(\"event_year\",  year(\"event_ts\"))\n",
    "      .withColumn(\"event_month\", month(\"event_ts\"))\n",
    "      .withColumn(\"event_day\",   dayofmonth(\"event_ts\"))\n",
    "      .withColumn(\"event_hour\",  hour(\"event_ts\"))\n",
    "      .withColumn(\"event_minute\", minute(\"event_ts\"))\n",
    "      .withColumn(\"event_second\", second(\"event_ts\"))\n",
    "      # Selección final en el orden deseado\n",
    "      .select(\n",
    "         \"partition_date\",\n",
    "         \"order_id\",\n",
    "         \"neighborhood\",\n",
    "         \"customer_id\",\n",
    "         \"employee_id\",\n",
    "         col(\"date\").alias(\"event_date\"),\n",
    "         \"event_day\",\n",
    "         \"event_hour\",\n",
    "         \"event_minute\",\n",
    "         \"event_month\",\n",
    "         \"event_second\",\n",
    "         \"event_year\",\n",
    "         \"latitude\",\n",
    "         \"longitude\",\n",
    "         \"district\",\n",
    "         \"quantity_products\"\n",
    "      )\n",
    ")\n",
    "\n",
    "# Verificar y mostrar\n",
    "print(\"✅ Bronze reordenado según el esquema original\")\n",
    "df_bronze.printSchema()\n",
    "df_bronze.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d90bf64-59d4-4d20-8229-c4fcbaf75705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Sección 6: Persistir en Delta y verificar\n",
    "# ==============================\n",
    "# Crea la base si no existe\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS unalwater\")\n",
    "\n",
    "(\n",
    "  df_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"partition_date\")\n",
    "    .saveAsTable(\"unalwater.bronze_events\")\n",
    ")\n",
    "\n",
    "# Verificación\n",
    "total = spark.table(\"unalwater.bronze_events\").count()\n",
    "print(f\"✅ Bronze listo. Total registros en tabla: {total}\")\n",
    "spark.table(\"unalwater.bronze_events\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cac943d-ac33-4ae0-8773-e7a2d88f82b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * \n",
    "FROM unalwater.bronze_events\n",
    "LIMIT 21;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "geopandas",
     "pyarrow",
     "numpy==1.24.4",
     "pandas==2.0.3"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8267139422616542,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_setup_and_load (Old)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
