{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85d221aa-f4dd-46ee-8434-8c5e25b3864c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 01_setup_and_simulate\n",
    "\n",
    "Este notebook:\n",
    "- Carga la configuración de simulación.\n",
    "- Define funciones de muestreo y generación.\n",
    "- Genera y guarda los JSON de eventos en DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a43319ef-6685-48e1-ba85-70a9f66457e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Imports y SparkSession\n",
    "# ==============================\n",
    "from pathlib import Path\n",
    "import json, random, uuid\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, DoubleType\n",
    ")\n",
    "\n",
    "# Inicia Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"01_setup_and_simulate\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3314a0d-1f97-4059-8728-b86bc0c27757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Carga de configuración y datos de insumo\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# 1) Parámetros de simulación\n",
    "cfg       = json.loads(Path(\"/Workspace/Users/santiagobustosp@gmail.com/medellin-bigdata-poc/sim_config.json\").read_text())\n",
    "base      = Path(cfg[\"base_path\"])\n",
    "raw_dir   = base / \"data\" / \"raw\"\n",
    "qty_min, qty_max = cfg[\"quantity_range\"]\n",
    "interval  = cfg[\"interval_seconds\"]\n",
    "\n",
    "# 2) Lectura de datos con pandas/geopandas\n",
    "pdf_cust  = pd.read_parquet(f\"{raw_dir}/customers.parquet\")\n",
    "pdf_emp   = pd.read_parquet(f\"{raw_dir}/employees.parquet\")\n",
    "gdf_neigh = gpd.read_parquet(f\"{raw_dir}/medellin_neighborhoods.parquet\")\n",
    "gdf_mask  = gpd.read_parquet(f\"{raw_dir}/50001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2002ee18-2ccf-45d4-9411-81837e6e074e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Preparar listas para sampling\n",
    "# ==============================\n",
    "from shapely.geometry import Point, shape\n",
    "\n",
    "# IDs para sampling\n",
    "cust_ids   = pdf_cust[\"customer_id\"].tolist()\n",
    "emp_ids    = pdf_emp[\"employee_id\"].tolist()\n",
    "\n",
    "# Geometrías para UDF espacial\n",
    "neigh_list = gdf_neigh.to_dict(\"records\")\n",
    "mask_geom  = shape(gdf_mask.loc[0, \"geometry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76eac53-c3dd-4c0d-8cc8-068762b83f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Celda 4 – Simulación de N eventos y persistencia en Delta\n",
    "# ==============================\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# 1) Generar lista de eventos\n",
    "N = 20\n",
    "events = []\n",
    "for _ in range(N):\n",
    "    b = random.choice(neigh_list)\n",
    "    minx, miny, maxx, maxy = shape(b[\"geometry\"]).bounds\n",
    "    while True:\n",
    "        lon = random.uniform(minx, maxx)\n",
    "        lat = random.uniform(miny, maxy)\n",
    "        pt = Point(lon, lat)\n",
    "        if shape(b[\"geometry\"]).contains(pt) and mask_geom.contains(pt):\n",
    "            break\n",
    "    events.append({\n",
    "        \"latitude\":           lat,\n",
    "        \"longitude\":          lon,\n",
    "        \"date\":               datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "        \"customer_id\":        random.choice(cust_ids),\n",
    "        \"employee_id\":        random.choice(emp_ids),\n",
    "        \"quantity_products\":  random.randint(qty_min, qty_max),\n",
    "        \"order_id\":           str(uuid.uuid4())\n",
    "    })\n",
    "\n",
    "# 2) Crear DataFrame Spark desde los eventos\n",
    "schema_ev = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), False),\n",
    "    StructField(\"longitude\", DoubleType(), False),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"employee_id\", IntegerType(), False),\n",
    "    StructField(\"quantity_products\", IntegerType(), False),\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "])\n",
    "df_raw = spark.createDataFrame(events, schema_ev)\n",
    "\n",
    "# 3) Persistir en Delta Lake como tabla temporal\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS unalwater\")\n",
    "(\n",
    "    df_raw\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"\")\n",
    "      .saveAsTable(\"unalwater.raw_events_temp\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "geopandas",
     "pyarrow",
     "numpy==1.24.4",
     "pandas==2.0.3"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8267139422616542,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_setup_and_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
